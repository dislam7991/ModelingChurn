{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a2b08e6-6b05-491d-8c72-4403bd9e96f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Final Pipeline: Reloading and Reprocessing All Data ---\n",
      "âœ… 1. Data loaded successfully.\n",
      "Aggregating historical price data...\n",
      "âœ… 2. Feature engineering and aggregation complete.\n",
      "âœ… 3. Data cleaning and encoding complete.\n",
      "\n",
      "=== Dataâ€‘Size Diagnostics ===\n",
      "Full dataframe shape: (16096, 457)\n",
      "Memory footprint (MB): 10.131351470947266\n",
      "Number of features: 456 (minus target)\n",
      "Number of samples: 16096\n",
      "Zeroâ€‘variance features: []\n",
      "âœ… 4. Data prep for modeling complete.\n",
      "\n",
      "--- Running Final Model Bake-off (excluding SVC) ---\n",
      "[2025-07-31 20:37:01.024300] ðŸš€ Starting Final Model Bakeâ€‘off (excluding SVC)â€¦\n",
      "[2025-07-31 20:37:01.548023] ðŸ•’ Beginning training of batch modelsâ€¦\n",
      "Training Dummy Classifier...\n",
      "Training Logistic Regression...\n",
      "Training Naive Bayes...\n",
      "Training KNN...\n",
      "Training Decision Tree...\n",
      "Training Random Forest...\n",
      "Training AdaBoost...\n",
      "Training XGBoost...\n",
      "Training LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 10876, number of negative: 10876\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005637 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6661\n",
      "[LightGBM] [Info] Number of data points in the train set: 21752, number of used features: 159\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FINAL, CORRECTED PIPELINE: From Raw Data to Final Model Evaluation\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "import os\n",
    "from datetime import datetime\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"6\"  # or \"12\"\n",
    "\n",
    "print(\"--- Starting Final Pipeline: Reloading and Reprocessing All Data ---\")\n",
    "\n",
    "# --- Part 1: Load Data ---\n",
    "try:\n",
    "    training_data = pd.read_csv('ml_case_training_data.csv')\n",
    "    output = pd.read_csv('ml_case_training_output.csv')\n",
    "    hist_data = pd.read_csv('ml_case_training_hist_data.csv')\n",
    "    print(\"âœ… 1. Data loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Could not find data files. {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Part 2: Merging, Aggregating, and Feature Engineering ---\n",
    "# Create a complete raw dataframe\n",
    "df_raw = pd.merge(pd.merge(training_data, output, on='id'), hist_data, on='id', how='left')\n",
    "\n",
    "# CORRECTED: Aggregate historical price data by customer ID\n",
    "print(\"Aggregating historical price data...\")\n",
    "df_raw['price_date'] = pd.to_datetime(df_raw['price_date'], errors='coerce')\n",
    "agg_price_data = df_raw.groupby('id').agg(\n",
    "    mean_price_p1_var=('price_p1_var', 'mean'),\n",
    "    std_price_p1_var=('price_p1_var', 'std'),\n",
    "    mean_price_p2_var=('price_p2_var', 'mean'),\n",
    "    std_price_p2_var=('price_p2_var', 'std'),\n",
    ").reset_index()\n",
    "\n",
    "# Create the main, unique customer dataframe `df`\n",
    "df = df_raw.drop(columns=hist_data.columns.drop('id')).drop_duplicates(subset='id')\n",
    "\n",
    "# Merge the aggregated price data back\n",
    "df = pd.merge(df, agg_price_data, on='id', how='left')\n",
    "\n",
    "# Feature Engineering\n",
    "df['date_activ'] = pd.to_datetime(df['date_activ'], errors='coerce')\n",
    "df['tenure'] = (pd.to_datetime('2016-01-01') - df['date_activ']).dt.days / 365.25\n",
    "df['margin_x_tenure'] = df['net_margin'] * df['tenure']\n",
    "print(\"âœ… 2. Feature engineering and aggregation complete.\")\n",
    "\n",
    "\n",
    "# --- Part 3: Data Cleaning and Final Prep ---\n",
    "# Drop unneeded columns\n",
    "cols_to_drop = [col for col in df.columns if 'date' in col or 'campaign_disc_ele' in col]\n",
    "df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "# Fix negative consumption errors\n",
    "for col in ['cons_12m', 'cons_gas_12m', 'cons_last_month', 'imp_cons']:\n",
    "    if col in df.columns:\n",
    "        df.loc[df[col] < 0, col] = np.nan\n",
    "\n",
    "# Convert binary 'has_gas'\n",
    "if 'has_gas' in df.columns and df['has_gas'].dtype == 'object':\n",
    "    df['has_gas'] = df['has_gas'].map({'t': 1, 'f': 0})\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "# **Drop the ID column** so it never gets oneâ€‘hotâ€‘encoded\n",
    "if 'id' in df.columns:\n",
    "    df.drop(columns=['id'], inplace=True)\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "\n",
    "# One-hot encode categoricals\n",
    "\n",
    "categorical_features = df.select_dtypes(include=['object']).columns\n",
    "df = pd.get_dummies(df, columns=categorical_features, drop_first=True)\n",
    "print(\"âœ… 3. Data cleaning and encoding complete.\")\n",
    "\n",
    "# --- Part 3.1: Quick Dataâ€‘Size Diagnostics ---\n",
    "print(\"\\n=== Dataâ€‘Size Diagnostics ===\")\n",
    "print(\"Full dataframe shape:\", df.shape)                            # rows Ã— columns\n",
    "print(\"Memory footprint (MB):\", df.memory_usage(deep=True).sum()/1024**2)\n",
    "print(\"Number of features:\", df.shape[1] - 1, \"(minus target)\")\n",
    "print(\"Number of samples:\", df.shape[0])\n",
    "# Optional: list zeroâ€‘variance features\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "vt = VarianceThreshold(threshold=0.0).fit(df.drop(columns='churn', errors='ignore'))\n",
    "zero_var = [c for c,v in zip(df.columns, vt.variances_) if v == 0]\n",
    "print(\"Zeroâ€‘variance features:\", zero_var)\n",
    "\n",
    "# --- Part 4: Final Split, Impute, and Balance ---\n",
    "X = df.drop(columns='churn', errors='ignore')\n",
    "y = df['churn']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "# Impute missing/infinite values\n",
    "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "median_values = X_train.median()\n",
    "X_train = X_train.fillna(median_values)\n",
    "X_test = X_test.fillna(median_values)\n",
    "\n",
    "# Balance with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "print(\"âœ… 4. Data prep for modeling complete.\")\n",
    "\n",
    "\n",
    "# --- Part 5: Model Training & Evaluation ---\n",
    "print(\"\\n--- Running Final Model Bake-off (excluding SVC) ---\")\n",
    "print(f\"[{datetime.now()}] ðŸš€ Starting Final Model Bakeâ€‘off (excluding SVC)â€¦\")\n",
    "\n",
    "# scale once\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# define SVC separately\n",
    "svc_model = SVC(random_state=42, probability=True)\n",
    "\n",
    "# all other models (no SVC in here)\n",
    "models = {\n",
    "    'Dummy Classifier': DummyClassifier(strategy='stratified', random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42, algorithm='SAMME'),\n",
    "    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    'LightGBM': LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# these models expect scaled data\n",
    "scaled_models = ['Logistic Regression', 'Naive Bayes', 'KNN']\n",
    "\n",
    "results = []\n",
    "\n",
    "print(f\"[{datetime.now()}] ðŸ•’ Beginning training of batch modelsâ€¦\")\n",
    "\n",
    "# 1) train & evaluate everyone except SVC\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    if name in scaled_models:\n",
    "        model.fit(X_train_scaled, y_train_resampled)\n",
    "        preds_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        preds      = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        model.fit(X_train_resampled, y_train_resampled)\n",
    "        preds_proba = model.predict_proba(X_test)[:, 1]\n",
    "        preds      = model.predict(X_test)\n",
    "\n",
    "    results.append({\n",
    "        'Model':     name,\n",
    "        'AUC':       roc_auc_score(y_test, preds_proba),\n",
    "        'Accuracy':  accuracy_score(y_test, preds),\n",
    "        'F1-Score':  f1_score(y_test, preds),\n",
    "        'Precision': precision_score(y_test, preds),\n",
    "        'Recall':    recall_score(y_test, preds)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17847680-6b36-42d5-b929-e38368864468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-31 20:37:21.959404] ðŸ•’ Finished batch models. Now training SVMâ€¦\n",
      "Training sklearn SVC on 497 featuresâ€¦\n",
      "\n",
      "--- Final Model Performance Comparison ---\n",
      "[2025-07-31 20:47:36.736636] ðŸŽ‰ All models complete. Here are the results:\n",
      "                          AUC  Accuracy  F1-Score  Precision    Recall\n",
      "Model                                                                 \n",
      "Random Forest        0.679437  0.891650  0.180451   0.360902  0.120301\n",
      "XGBoost              0.673428  0.891402  0.215440   0.379747  0.150376\n",
      "LightGBM             0.664376  0.890656  0.179104   0.350365  0.120301\n",
      "AdaBoost             0.628777  0.822813  0.215622   0.192157  0.245614\n",
      "SVC                  0.601714  0.837227  0.146023   0.152174  0.140351\n",
      "KNN                  0.589034  0.791252  0.200000   0.161290  0.263158\n",
      "Logistic Regression  0.575898  0.823062  0.183486   0.169133  0.200501\n",
      "Decision Tree        0.534871  0.802932  0.167891   0.144404  0.200501\n",
      "Dummy Classifier     0.524482  0.508946  0.180083   0.107907  0.543860\n",
      "Naive Bayes          0.488675  0.141153  0.175573   0.097021  0.922306\n",
      "\n",
      "--- SCRIPT COMPLETE ---\n"
     ]
    }
   ],
   "source": [
    "# --- Part 5 (modified SVC) ---\n",
    "# Run SVC separately due to time requirements\n",
    "print(f\"[{datetime.now()}] ðŸ•’ Finished batch models. Now training SVMâ€¦\")\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# pass these params into SVC to speed up training\n",
    "svc = SVC(\n",
    "    kernel='rbf',\n",
    "    cache_size=2048,\n",
    "    probability=True,\n",
    "    tol=1e-3,\n",
    "    random_state=42\n",
    ")\n",
    "print(\"Training sklearn SVC on 497 featuresâ€¦\")\n",
    "svc.fit(X_train_scaled, y_train_resampled)\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "y_proba = svc.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "results.append({\n",
    "    'Model':    'SVC',\n",
    "    'AUC':      roc_auc_score(y_test, y_proba) if 'y_proba' in locals() else None,\n",
    "    'F1-Score': f1_score(y_test, y_pred),\n",
    "    'Recall':   recall_score(y_test, y_pred),\n",
    "    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "    'Precision':precision_score(y_test, y_pred)\n",
    "})\n",
    "\n",
    "\n",
    "# 3) compile and display\n",
    "results_df = (\n",
    "    pd.DataFrame(results)\n",
    "      .sort_values(by='AUC', ascending=False)\n",
    "      .set_index('Model')\n",
    ")\n",
    "\n",
    "print(\"\\n--- Final Model Performance Comparison ---\")\n",
    "print(f\"[{datetime.now()}] ðŸŽ‰ All models complete. Here are the results:\")\n",
    "print(results_df)\n",
    "print(\"\\n--- SCRIPT COMPLETE ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68542eed-410e-4771-8374-3df48de1206f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
